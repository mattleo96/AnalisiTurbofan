{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Remaining Useful Life Prediction of Turbofan Engine Using Hybrid Model Based on Autoencoder and Bidirectional Long Short-Term Memory[Autoencoder].ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRxxJYis9YEe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "961062fb-e212-4bea-cfc2-72261ecee405"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.layers import LSTM,Dropout,GRU,MaxPooling1D,Flatten,concatenate\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Concatenate,Bidirectional\n",
        "from keras.layers.convolutional import Conv1D\n",
        "import tensorflow as tf\n",
        "from keras.utils.vis_utils import model_to_dot\n",
        "from keras.layers import RepeatVector\n",
        "from keras.layers import TimeDistributed,Activation\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from keras import Input,Model\n",
        "\n",
        "FD1_train = pd.read_csv(\"train_FD001.txt\",delimiter=\" \",header = None)\n",
        "FD1_test = pd.read_csv(\"test_FD001.txt\",delimiter=\" \",header = None)\n",
        "RUL_file=pd.read_csv(\"RUL_FD001.txt\",header = None)\n",
        "##L' unica differenza con la maggior parte dei prerpcessing degli altri articoli e che non vengono effettuati drop delle feature perchè il processo di feature engineering è svolto dall' autoencoder\n",
        "FD1_test=FD1_test.drop([26, 27], axis = 1)\n",
        "columns = {0:'unit_number',1:'time_in_cycles',2:'opSetting1',3:'opSetting2',4:'opSetting3',5:'sensor1',6:'sensor2',\n",
        "           7:'sensor3',8:'sensor4',9:'sensor5',10:'sensor6',11:'sensor7',12:'sensor8',13:'sensor9',14:'sensor10',\n",
        "           15:'sensor11',16:'sensor12',17:'sensor13',18:'sensor14',19:'sensor15',20:'sensor16',\n",
        "           21:'sensor17',22:'sensor18',23:'sensor19',24:'sensor20',25:'sensor21'}\n",
        "FD1_test.columns=columns\n",
        "FD1_test = FD1_test.rename(columns=columns)\n",
        "#FD1_test=FD1_test.drop(columns = ['opSetting1','opSetting2','opSetting3','sensor1','sensor5','sensor6','sensor10','sensor16','sensor18','sensor19','sensor17'], axis =1)\n",
        "\n",
        "\n",
        "\n",
        "FD1_train=FD1_train.drop([26, 27], axis = 1)\n",
        "columns = {0:'unit_number',1:'time_in_cycles',2:'opSetting1',3:'opSetting2',4:'opSetting3',5:'sensor1',6:'sensor2',\n",
        "           7:'sensor3',8:'sensor4',9:'sensor5',10:'sensor6',11:'sensor7',12:'sensor8',13:'sensor9',14:'sensor10',\n",
        "           15:'sensor11',16:'sensor12',17:'sensor13',18:'sensor14',19:'sensor15',20:'sensor16',\n",
        "           21:'sensor17',22:'sensor18',23:'sensor19',24:'sensor20',25:'sensor21'}\n",
        "FD1_train = FD1_train.rename(columns=columns)\n",
        "#FD1_train=FD1_train.drop(columns = ['opSetting1','opSetting2','opSetting3','sensor1','sensor5','sensor6','sensor10','sensor16','sensor18','sensor19','sensor17'], axis =1)\n",
        "\n",
        "# calcolo RUL\n",
        "def Calcolo_RUL(df): \n",
        "    df_copy=df.copy()\n",
        "    gruppo=df_copy.groupby('unit_number')['time_in_cycles'].max().reset_index()\n",
        "    gruppo.columns = ['unit_number','max_cycles_fu']\n",
        "    df_copy = df_copy.merge(gruppo, on=['unit_number'], how='left')\n",
        "    df['RUL'] = df_copy['max_cycles_fu'] - df_copy['time_in_cycles']\n",
        "    return df\n",
        "Calcolo_RUL(FD1_train)\n",
        "#normalizzazione train test\n",
        "cols_normalize = FD1_train.columns.difference( #non considero unit_number,time_in_cycles,RUL per normalizzazione\n",
        "    ['unit_number', 'time_in_cycles', 'RUL'])\n",
        "  \n",
        "\n",
        "min_max_scaler = MinMaxScaler()\n",
        "\n",
        "norm_train_df = pd.DataFrame(min_max_scaler.fit_transform(FD1_train[cols_normalize]),\n",
        "                             columns=cols_normalize,\n",
        "                             index=FD1_train.index)\n",
        "\n",
        "join_df = FD1_train[FD1_train.columns.difference(cols_normalize)].join(norm_train_df)\n",
        "FD1_train = join_df.reindex(columns=FD1_train.columns)\n",
        "\n",
        "cols_normalizetest = FD1_test.columns.difference(  #non considero unit_number,time_in_cyclesper normalizzazione\n",
        "    ['unit_number', 'time_in_cycles'])  \n",
        "\n",
        "norm_train_df_test = pd.DataFrame(min_max_scaler.transform(FD1_test[cols_normalizetest]),\n",
        "                             columns=cols_normalizetest,\n",
        "                             index=FD1_test.index)\n",
        "\n",
        "join_df = FD1_test[FD1_test.columns.difference(cols_normalizetest)].join(norm_train_df_test)\n",
        "FD1_test = join_df.reindex(columns=FD1_test.columns)\n",
        "FD1_test=FD1_test.drop(columns = ['time_in_cycles'], axis =1)\n",
        "data_cols= FD1_test.columns\n",
        "##\n",
        "## reshape dati test\n",
        "def prepare_test_data(df):\n",
        "    data_list = []\n",
        "    for unit_number in df.unit_number.unique():\n",
        "        unit = df[df.unit_number == unit_number]\n",
        "        data_list.append(np.array(unit[data_cols])[-31:, :])\n",
        "    return np.stack(data_list)\n",
        "FD1_test=prepare_test_data(FD1_test)\n",
        "FD1_test=FD1_test[:,:,1:]\n",
        "\n",
        "\n",
        "## upper bound per RUL\n",
        "rul_clip_limit = 120\n",
        "FD1_train=FD1_train.clip(upper=rul_clip_limit)\n",
        "\n",
        "FD1_c=FD1_train.copy()\n",
        "FD1_train = FD1_train.drop(['RUL','time_in_cycles'], axis=1)\n",
        "## reshape dati di train\n",
        "def gen_sequence(id_df, seq_length, seq_cols):\n",
        "    data_array = id_df[seq_cols].values\n",
        "    num_elements = data_array.shape[0]\n",
        "    for start, stop in zip(range(0, num_elements-seq_length), range(seq_length, num_elements)):\n",
        "        yield data_array[start:stop, :]\n",
        "\n",
        "seq_gen = (list(gen_sequence(FD1_train[FD1_train['unit_number']==unit_number],31, FD1_train.columns))\n",
        "           for unit_number in FD1_train['unit_number'].unique())\n",
        "\n",
        "seq_array = np.concatenate(list(seq_gen)).astype(np.float32)\n",
        "\n",
        "\n",
        "seq_array=seq_array[:,:,1:] #elimino unit_number dal train\n",
        "print(seq_array.shape) # dimensione dei dati di train\n",
        "\n",
        "def gen_labels(id_df, seq_length, label):\n",
        "    data_array = id_df[label].values\n",
        "    num_elements = data_array.shape[0]\n",
        "    return data_array[seq_length:num_elements, :]\n",
        "\n",
        "label_gen = [gen_labels(FD1_c[FD1_c['unit_number']==id], 31, ['RUL'])\n",
        "             for id in FD1_c['unit_number'].unique()]\n",
        "label_array = np.concatenate(label_gen).astype(np.float32)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "a=Input(shape=(31,24))\n",
        "encoded=(LSTM(12,return_sequences=True))(a)\n",
        "decoded=(LSTM(24,return_sequences=True))(encoded)\n",
        "autoencoder = Model(a, decoded)\n",
        "encoder = Model(a, encoded)\n",
        "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "autoencoder.fit(seq_array,seq_array,batch_size=50,epochs=50,validation_split=0.10,verbose=1)\n",
        "en=encoder.predict(seq_array)\n",
        "FD1_test=encoder.predict(FD1_test)\n",
        "from numpy import var\n",
        "\n",
        "\n",
        "model= Sequential()\n",
        "model.add(Bidirectional(LSTM(100,input_shape=(31,12), activation='tanh', return_sequences=True)))\n",
        "model.add(Bidirectional(LSTM(50,input_shape=(31,12), activation='tanh',return_sequences=False)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(30, activation='relu'))\n",
        "model.add(Dense(1, activation='linear'))\n",
        "model.compile(optimizer='RMSprop', loss='mse',metrics = [tf.keras.metrics.RootMeanSquaredError()])\n",
        "model.fit(en,label_array ,batch_size=200,epochs=32,verbose=1)\n",
        "y_pred= model.predict(FD1_test)\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from math import sqrt\n",
        "RUL_file=pd.read_csv(\"RUL_FD001.txt\",header = None)\n",
        "print(sqrt(mean_squared_error(RUL_file, y_pred)))\n",
        "print(y_pred)\n",
        "print((mean_squared_error(RUL_file, y_pred)))\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(17531, 31, 24)\n",
            "Epoch 1/50\n",
            "316/316 [==============================] - 5s 8ms/step - loss: 1.5465 - val_loss: 0.6920\n",
            "Epoch 2/50\n",
            "316/316 [==============================] - 2s 5ms/step - loss: 0.6953 - val_loss: 0.6731\n",
            "Epoch 3/50\n",
            "316/316 [==============================] - 2s 5ms/step - loss: 0.6778 - val_loss: 0.6606\n",
            "Epoch 4/50\n",
            "316/316 [==============================] - 2s 5ms/step - loss: 0.6672 - val_loss: 0.6555\n",
            "Epoch 5/50\n",
            "316/316 [==============================] - 2s 6ms/step - loss: 0.6628 - val_loss: 0.6527\n",
            "Epoch 6/50\n",
            "316/316 [==============================] - 2s 5ms/step - loss: 0.6590 - val_loss: 0.6504\n",
            "Epoch 7/50\n",
            "316/316 [==============================] - 2s 5ms/step - loss: 0.6587 - val_loss: 0.6494\n",
            "Epoch 8/50\n",
            "316/316 [==============================] - 2s 6ms/step - loss: 0.6567 - val_loss: 0.6484\n",
            "Epoch 9/50\n",
            "316/316 [==============================] - 2s 5ms/step - loss: 0.6573 - val_loss: 0.6475\n",
            "Epoch 10/50\n",
            "316/316 [==============================] - 2s 5ms/step - loss: 0.6551 - val_loss: 0.6463\n",
            "Epoch 11/50\n",
            "316/316 [==============================] - 2s 5ms/step - loss: 0.6545 - val_loss: 0.6448\n",
            "Epoch 12/50\n",
            "316/316 [==============================] - 2s 6ms/step - loss: 0.6527 - val_loss: 0.6434\n",
            "Epoch 13/50\n",
            "316/316 [==============================] - 2s 6ms/step - loss: 0.6494 - val_loss: 0.6421\n",
            "Epoch 14/50\n",
            "316/316 [==============================] - 2s 5ms/step - loss: 0.6477 - val_loss: 0.6410\n",
            "Epoch 15/50\n",
            "316/316 [==============================] - 2s 5ms/step - loss: 0.6473 - val_loss: 0.6402\n",
            "Epoch 16/50\n",
            "316/316 [==============================] - 2s 5ms/step - loss: 0.6469 - val_loss: 0.6397\n",
            "Epoch 17/50\n",
            "316/316 [==============================] - 2s 5ms/step - loss: 0.6460 - val_loss: 0.6393\n",
            "Epoch 18/50\n",
            "316/316 [==============================] - 2s 5ms/step - loss: 0.6469 - val_loss: 0.6389\n",
            "Epoch 19/50\n",
            "316/316 [==============================] - 2s 5ms/step - loss: 0.6453 - val_loss: 0.6384\n",
            "Epoch 20/50\n",
            "316/316 [==============================] - 2s 5ms/step - loss: 0.6448 - val_loss: 0.6379\n",
            "Epoch 21/50\n",
            "316/316 [==============================] - 2s 5ms/step - loss: 0.6445 - val_loss: 0.6374\n",
            "Epoch 22/50\n",
            "316/316 [==============================] - 2s 5ms/step - loss: 0.6438 - val_loss: 0.6371\n",
            "Epoch 23/50\n",
            "316/316 [==============================] - 2s 6ms/step - loss: 0.6443 - val_loss: 0.6367\n",
            "Epoch 24/50\n",
            "316/316 [==============================] - 2s 6ms/step - loss: 0.6446 - val_loss: 0.6364\n",
            "Epoch 25/50\n",
            "316/316 [==============================] - 2s 6ms/step - loss: 0.6427 - val_loss: 0.6361\n",
            "Epoch 26/50\n",
            "316/316 [==============================] - 2s 6ms/step - loss: 0.6438 - val_loss: 0.6359\n",
            "Epoch 27/50\n",
            "316/316 [==============================] - 2s 6ms/step - loss: 0.6425 - val_loss: 0.6356\n",
            "Epoch 28/50\n",
            "316/316 [==============================] - 2s 5ms/step - loss: 0.6416 - val_loss: 0.6355\n",
            "Epoch 29/50\n",
            "316/316 [==============================] - 2s 5ms/step - loss: 0.6413 - val_loss: 0.6353\n",
            "Epoch 30/50\n",
            "316/316 [==============================] - 2s 5ms/step - loss: 0.6425 - val_loss: 0.6351\n",
            "Epoch 31/50\n",
            "316/316 [==============================] - 2s 5ms/step - loss: 0.6426 - val_loss: 0.6348\n",
            "Epoch 32/50\n",
            "316/316 [==============================] - 2s 5ms/step - loss: 0.6407 - val_loss: 0.6346\n",
            "Epoch 33/50\n",
            "316/316 [==============================] - 2s 5ms/step - loss: 0.6407 - val_loss: 0.6343\n",
            "Epoch 34/50\n",
            "316/316 [==============================] - 2s 5ms/step - loss: 0.6414 - val_loss: 0.6341\n",
            "Epoch 35/50\n",
            "316/316 [==============================] - 2s 5ms/step - loss: 0.6394 - val_loss: 0.6338\n",
            "Epoch 36/50\n",
            "316/316 [==============================] - 2s 5ms/step - loss: 0.6392 - val_loss: 0.6336\n",
            "Epoch 37/50\n",
            "316/316 [==============================] - 2s 5ms/step - loss: 0.6401 - val_loss: 0.6334\n",
            "Epoch 38/50\n",
            "316/316 [==============================] - 2s 5ms/step - loss: 0.6393 - val_loss: 0.6332\n",
            "Epoch 39/50\n",
            "316/316 [==============================] - 2s 5ms/step - loss: 0.6387 - val_loss: 0.6331\n",
            "Epoch 40/50\n",
            "316/316 [==============================] - 2s 6ms/step - loss: 0.6392 - val_loss: 0.6329\n",
            "Epoch 41/50\n",
            "316/316 [==============================] - 2s 5ms/step - loss: 0.6404 - val_loss: 0.6328\n",
            "Epoch 42/50\n",
            "316/316 [==============================] - 2s 5ms/step - loss: 0.6394 - val_loss: 0.6326\n",
            "Epoch 43/50\n",
            "316/316 [==============================] - 2s 5ms/step - loss: 0.6380 - val_loss: 0.6325\n",
            "Epoch 44/50\n",
            "316/316 [==============================] - 2s 5ms/step - loss: 0.6381 - val_loss: 0.6324\n",
            "Epoch 45/50\n",
            "316/316 [==============================] - 2s 5ms/step - loss: 0.6384 - val_loss: 0.6323\n",
            "Epoch 46/50\n",
            "316/316 [==============================] - 2s 6ms/step - loss: 0.6394 - val_loss: 0.6322\n",
            "Epoch 47/50\n",
            "316/316 [==============================] - 2s 6ms/step - loss: 0.6382 - val_loss: 0.6322\n",
            "Epoch 48/50\n",
            "316/316 [==============================] - 2s 5ms/step - loss: 0.6378 - val_loss: 0.6321\n",
            "Epoch 49/50\n",
            "316/316 [==============================] - 2s 5ms/step - loss: 0.6388 - val_loss: 0.6321\n",
            "Epoch 50/50\n",
            "316/316 [==============================] - 2s 5ms/step - loss: 0.6386 - val_loss: 0.6321\n",
            "Epoch 1/32\n",
            "88/88 [==============================] - 9s 13ms/step - loss: 5109.3128 - root_mean_squared_error: 77.6114\n",
            "Epoch 2/32\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 1507.7537 - root_mean_squared_error: 57.9646\n",
            "Epoch 3/32\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 469.9723 - root_mean_squared_error: 46.9043\n",
            "Epoch 4/32\n",
            "88/88 [==============================] - 1s 14ms/step - loss: 398.5188 - root_mean_squared_error: 40.6752\n",
            "Epoch 5/32\n",
            "88/88 [==============================] - 1s 14ms/step - loss: 386.2565 - root_mean_squared_error: 36.8254\n",
            "Epoch 6/32\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 341.8585 - root_mean_squared_error: 34.1451\n",
            "Epoch 7/32\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 313.7334 - root_mean_squared_error: 32.1351\n",
            "Epoch 8/32\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 281.4269 - root_mean_squared_error: 30.5202\n",
            "Epoch 9/32\n",
            "88/88 [==============================] - 1s 14ms/step - loss: 277.8306 - root_mean_squared_error: 29.2145\n",
            "Epoch 10/32\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 267.3722 - root_mean_squared_error: 28.1219\n",
            "Epoch 11/32\n",
            "88/88 [==============================] - 1s 14ms/step - loss: 259.1889 - root_mean_squared_error: 27.1917\n",
            "Epoch 12/32\n",
            "88/88 [==============================] - 1s 14ms/step - loss: 250.8818 - root_mean_squared_error: 26.3864\n",
            "Epoch 13/32\n",
            "88/88 [==============================] - 1s 14ms/step - loss: 243.5257 - root_mean_squared_error: 25.6760\n",
            "Epoch 14/32\n",
            "88/88 [==============================] - 1s 14ms/step - loss: 229.3428 - root_mean_squared_error: 25.0443\n",
            "Epoch 15/32\n",
            "88/88 [==============================] - 1s 14ms/step - loss: 228.4830 - root_mean_squared_error: 24.4768\n",
            "Epoch 16/32\n",
            "88/88 [==============================] - 1s 14ms/step - loss: 210.4713 - root_mean_squared_error: 23.9545\n",
            "Epoch 17/32\n",
            "88/88 [==============================] - 1s 14ms/step - loss: 204.4057 - root_mean_squared_error: 23.4804\n",
            "Epoch 18/32\n",
            "88/88 [==============================] - 1s 14ms/step - loss: 199.1097 - root_mean_squared_error: 23.0485\n",
            "Epoch 19/32\n",
            "88/88 [==============================] - 1s 14ms/step - loss: 196.1971 - root_mean_squared_error: 22.6470\n",
            "Epoch 20/32\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 188.5219 - root_mean_squared_error: 22.2724\n",
            "Epoch 21/32\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 179.5401 - root_mean_squared_error: 21.9241\n",
            "Epoch 22/32\n",
            "88/88 [==============================] - 1s 14ms/step - loss: 173.4430 - root_mean_squared_error: 21.5979\n",
            "Epoch 23/32\n",
            "88/88 [==============================] - 1s 14ms/step - loss: 173.3349 - root_mean_squared_error: 21.2900\n",
            "Epoch 24/32\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 166.7543 - root_mean_squared_error: 21.0028\n",
            "Epoch 25/32\n",
            "88/88 [==============================] - 1s 14ms/step - loss: 164.6613 - root_mean_squared_error: 20.7317\n",
            "Epoch 26/32\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 160.4950 - root_mean_squared_error: 20.4744\n",
            "Epoch 27/32\n",
            "88/88 [==============================] - 1s 14ms/step - loss: 159.3532 - root_mean_squared_error: 20.2345\n",
            "Epoch 28/32\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 154.8708 - root_mean_squared_error: 20.0065\n",
            "Epoch 29/32\n",
            "88/88 [==============================] - 1s 14ms/step - loss: 157.8797 - root_mean_squared_error: 19.7908\n",
            "Epoch 30/32\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 150.6489 - root_mean_squared_error: 19.5849\n",
            "Epoch 31/32\n",
            "88/88 [==============================] - 1s 14ms/step - loss: 146.2298 - root_mean_squared_error: 19.3872\n",
            "Epoch 32/32\n",
            "88/88 [==============================] - 1s 14ms/step - loss: 149.3383 - root_mean_squared_error: 19.2008\n",
            "13.593295512933414\n",
            "[[115.17821  ]\n",
            " [115.63476  ]\n",
            " [ 55.022305 ]\n",
            " [ 94.34033  ]\n",
            " [ 93.4658   ]\n",
            " [102.726616 ]\n",
            " [106.82274  ]\n",
            " [ 86.242516 ]\n",
            " [117.97284  ]\n",
            " [ 83.945015 ]\n",
            " [ 75.17152  ]\n",
            " [111.32024  ]\n",
            " [105.54395  ]\n",
            " [108.92584  ]\n",
            " [106.024475 ]\n",
            " [101.01565  ]\n",
            " [ 45.65489  ]\n",
            " [ 27.080149 ]\n",
            " [103.22697  ]\n",
            " [ 18.908953 ]\n",
            " [ 70.351456 ]\n",
            " [116.453835 ]\n",
            " [116.418434 ]\n",
            " [ 24.729069 ]\n",
            " [115.02739  ]\n",
            " [112.10407  ]\n",
            " [ 67.169945 ]\n",
            " [115.08365  ]\n",
            " [111.48108  ]\n",
            " [107.34567  ]\n",
            " [  8.546837 ]\n",
            " [ 55.44023  ]\n",
            " [112.985535 ]\n",
            " [  2.5984266]\n",
            " [ 11.619738 ]\n",
            " [ 26.662094 ]\n",
            " [ 23.254446 ]\n",
            " [ 64.67383  ]\n",
            " [119.63684  ]\n",
            " [ 33.71693  ]\n",
            " [ 23.001987 ]\n",
            " [  7.449874 ]\n",
            " [ 68.47615  ]\n",
            " [ 93.48979  ]\n",
            " [ 72.935684 ]\n",
            " [ 40.950348 ]\n",
            " [113.41896  ]\n",
            " [112.89805  ]\n",
            " [ 17.46885  ]\n",
            " [ 93.86184  ]\n",
            " [104.010735 ]\n",
            " [ 34.338818 ]\n",
            " [ 32.747623 ]\n",
            " [117.415215 ]\n",
            " [113.8223   ]\n",
            " [ 15.699763 ]\n",
            " [101.075874 ]\n",
            " [ 50.86555  ]\n",
            " [111.82048  ]\n",
            " [104.26637  ]\n",
            " [ 16.175333 ]\n",
            " [ 44.16592  ]\n",
            " [ 82.94331  ]\n",
            " [ 19.522705 ]\n",
            " [119.31504  ]\n",
            " [ 13.326886 ]\n",
            " [109.60855  ]\n",
            " [  8.659928 ]\n",
            " [114.68012  ]\n",
            " [ 98.885284 ]\n",
            " [107.50094  ]\n",
            " [ 64.85452  ]\n",
            " [111.59099  ]\n",
            " [106.30652  ]\n",
            " [114.39313  ]\n",
            " [  4.1368876]\n",
            " [ 28.314157 ]\n",
            " [120.211426 ]\n",
            " [ 73.15093  ]\n",
            " [ 99.02211  ]\n",
            " [  4.334687 ]\n",
            " [  6.9858437]\n",
            " [116.86688  ]\n",
            " [ 67.74661  ]\n",
            " [118.11691  ]\n",
            " [113.33327  ]\n",
            " [114.1956   ]\n",
            " [115.72671  ]\n",
            " [102.82309  ]\n",
            " [ 32.602894 ]\n",
            " [ 25.428658 ]\n",
            " [ 23.838505 ]\n",
            " [ 51.121403 ]\n",
            " [ 64.44199  ]\n",
            " [113.05953  ]\n",
            " [113.401024 ]\n",
            " [ 97.67735  ]\n",
            " [ 66.74647  ]\n",
            " [118.883675 ]\n",
            " [ 16.702166 ]]\n",
            "184.77768290193572\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}